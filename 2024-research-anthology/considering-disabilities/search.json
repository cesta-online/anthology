[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Considering Disabilities in Online Cultural Experiences",
    "section": "",
    "text": "Project Members\n        \n        \n          \n            Project Member\n          \n          \n            Designation\n          \n          \n            ORCID\n          \n        \n            \n          \n            Patricia Alessandrini\n          \n          \n            Assistant Professor of Music\n          \n          0000-0000-0000-0000\n        \n            \n          \n            Chi Pham\n          \n          \n            Intern - Summer, 2024\n          \n          0000-0000-0000-0000\n        \n          \n        \n      \n\n      \n  \n  Application of AI Tools in Music Generation\n  \n  Chi Pham\n  \n  During the Summer of 2024, I had the privilege of working with Professor Alessandrini on the “Considering Disabilities in Online Cultural Experiences” project. Drawing from my background in Design and a strong interest in accessibility, I focused on researching the application of AI tools in music generation.\n  Traditional methods of composing music present barriers for Disabled individuals, such as issues with instrument design and mobility. AI has the potential to overcome these challenges and revolutionize the music industry. For instance, a user with partial vocal ability can use tools like Somax2 to compose a song complete with chords and instrumentals. My research reveals the potential of AI-generated music to be commercially popular. A recent example is a popular song by American Producer Metro Boomin, “BBL Drizzy”, which was revealed to have sampled from an audio track created by Udio, a text-to-music AI tool. The voice leading the song was completely AI-generated, as well as a part of the song. As of August of 2024, the track currently has a little bit more than 7 million streams on SoundCloud.\n  \n  \n  \n  Fig 1. Four song samples generated from the prompt “ancient Greece, Greek gods, happy, normal tempo, classical” using Udio\n  \n  \n  The research was divided into two categories: online and offline tools. Online tools, such as Somax2 and Prism Lab, facilitate live, interactive co-creation between humans and machines. I analyzed their training datasets, exploring the diversity and performance of their musical outputs. For offline tools, which generate music from text inputs, I assessed six AI music-generation tools, evaluating the ethics of their training datasets, generation time, and overall performance. This ethical consideration in training datasets is particularly important as AI companies like Suno and Udio are facing multimillion-dollar lawsuits from music labels over copyright infringement (Wired).\n  \n  \n  \n  Fig 2. Benny Sluchin playing the trombone (left) performing with Mikhail Malt monitoring Somax2 (right)\n  \n  \n  \n  \n  Visually Expressing Emotions Online\n  \n  Chi Pham\n  \n  My second project of the summer involved the Muse-IT online co-creation platform, which aims to recreate the in-person co-creating process. Currently, one thing that is lacking on the platform is the communication of emotions between musicians. To address this gap, I worked on developing a way to express emotions online through the use of “avatars.” This initiative is intended to be a co-creation process with the participants, and we are currently designing a workshop in Sweden for November to gather participants’ feedback. For example, I proposed a rapid prototyping workshop where the participants will use materials provided like clay, post-it notes, pen and paper, to design how they want their emotions to be visually expressed. This workshop will be crucial in refining the platform, ensuring that it truly meets the needs and desires of the participants, particularly in conveying the emotional nuances that are often lost in digital collaboration.\n  In a previous workshop, participants sketched out their ideas on paper, which I then digitized using Figma. These designs included various examples of humanoid avatars where facial expressions and limb movements could change to express emotions, as well as non-humanoid representations like lines with different colors and dynamics to convey the energy of the participants. Emotions in this context were organized by arousal—how excited or relaxed someone is—and valence—how happy or sad someone is.\n  \n  \n  Fig 3. Figma Mock-Up of “Avatars” First Drawn Out by Participants"
  }
]