[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Human Rights-Based Accountability Framework for Halting Government Hacking Abuses",
    "section": "",
    "text": "Project Members\n        \n        \n          \n            Project Member\n          \n          \n            Designation\n          \n          \n            ORCID\n          \n        \n            \n          \n            Andrè Ramiro\n          \n          \n            DCSL/CCSRE Practitioner Fellow, Stanford PACS\n          \n          \n        \n            \n          \n            Bridget Algee-Hewitt\n          \n          \n            Senior Associate Director of the Research Institute, CCSRE\n          \n          \n        \n            \n          \n            Ishani Mukherjee\n          \n          \n            Intern, Summer 2024\n          \n          \n        \n          \n        \n      \n\n      \n  \n  A Quantitative Approach to Hacking Abuses\n  \n  Ishani Mukherjee\n  \n  \n  Introduction\n  In the Digital World, data is a valuable currency. This makes the protection of data from compromise by malicious third parties an important priority for individuals, organizations and governments alike. But what if government entities are themselves the source of the threat? We define government hacking as instances where hacking technologies, such as spyware or vulnerabilities in software, are exploited by intelligence and law enforcement authorities to gather data for intelligence and for the prosecution of criminal activities. Currently, there is no robust database that documents legal cases that involve government hacking in the United States. This can be attributed to the fact that legal databases are created largely through manual docuemnt review methods, making the process of compiling them time- consuming and laborious.\n  \n  \n  Project Work\n  In my project for the summer, I experimented with using Large Language Models (LLMs) to streamline the extraction of attributes from legal cases involving instances of government hacking in the U.S. Currently, legal databases are created largely through manual methods, making the process of compiling them time-consuming and laborious. Using HeinOnline as my primary source, I scraped PDFs of relevant legal cases by building a web scraper and converted them into text files. I then fed these text files to different LLMs, specifically Gemini and GPT, to extract key attributes: the plaintiff, defendant, citation code, date argued, place of filing, and legal cases referenced in the document.\n  \n  \n  \n  Fig. 1. GPT Extraction Example\n  \n  \n  To ensure high accuracy, I experimented with both zero-shot (that is, without providing any in-context examples of the task to be executed) and one-shot prompting techniques. I built a comprehensive evaluation pipeline to test the accuracy of various extraction iterations, leading to the creation of a database that encapsulates all the extracted attributes in an easily digestible format that can be modified manually as required. This greatly reduces the complexity of the manual work involved in database creation, changing the task from one that involves extensive compilation to a simpler one that only entails occasional manual data entry.\n  \n  \n  Visualization\n  To ensure high accuracy, I experimented with both zero-shot (that is, without providing any in-context examples of the task to be executed) and one-shot prompting techniques. I built a comprehensive evaluation pipeline to test the accuracy of various extraction iterations, leading to the creation of a database that encapsulates all the extracted attributes in an easily digestible format that can be modified manually as required. This greatly reduces the complexity of the manual work involved in database creation, changing the task from one that involves extensive compilation to a simpler one that only entails occasional manual data entry. Beyond data extraction, I also experimented with data visualization. I created visualizations highlighting the most frequent legal citations in the database, as well as the most frequent filing districts. This led to some significant insights into the data – I was able to identify, for instance, that D.C, Northern California and New York were prominent centers of litigation.\n  \n  \n  \n  Fig. 2. Most Frequent Filing Districts\n  \n  \n  \n  \n  \n  Fig. 3. Most Frequently Cited Cases\n  \n  \n  \n  \n  Mapping\n  I then created two maps using ArcGIS to represent the geographical distribution of the originally scraped legal cases (not including the citations within them). The first plots the filing district of each case, with the size of the coordinate scaled according to the number of times that case was cited within the database.\n  \n  \n  \n  \n  \n  The second is a heat map that represents the districts with the most number of cases filed.\n  \n  \n  \n  \n  \n  The maps led to some interesting observations. Most notably, the similarity between them seems to suggest that there is a positive correlation between the more frequent filing districts and the cases with higher citation counts.\n  \n  \n  Significance\n  The creation of this database is in itself a significant step forward to better understanding the legal policy surrounding government hacking. By utilizing LLMs, the process of creating a legal database has been greatly simplified. The experiments revealed that LLMs effectively capture a substantial portion of citations within legal cases (which can be attributed to the fact that legal cases are highly structured documents), reducing the need for manual document review. This integration of a quantitative approach to legal research not only enhances efficiency but also opens new avenues for analyzing legal trends and patterns, since the approach is generalizable to any subfield within legal studies.\n  \n  \n  Future Directions\n  Given that the dataset is of a respectable size, there are several directions for future research:\n  \n  Outcome Analysis: Including the outcome of each case (i.e as favoring the plaintiff/defendant on a numerical scale that quantifies what portion of the charges against the defendant were dropped or upheld) as part of the dataset\n  Model Training and Outcome Prediction: Training a regression model on the dataset after adding outcomes and then testing it on the task of predicting the result of lawsuits related to government hacking that are currently in litigation\n  Extending to Different Legal Systems: Extending the methodology to cover legal systems in different countries (provided that the language differences are appropriately handled). This could be a useful tool to compare the laws governing government hacking practices across countries."
  }
]