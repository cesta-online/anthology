[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Modernist Archives Publishing Project",
    "section": "",
    "text": "Project Members\n        \n        \n          \n            Project Member\n          \n          \n            Designation\n          \n          \n            ORCID\n          \n        \n            \n          \n            Dr. Alice Staveley\n          \n          \n            Senior Lecturer in the English Department\n          \n          0000-0000-0000-0000\n        \n            \n          \n            Sophie Wu\n          \n          \n            Intern - Summer, 2024\n          \n          0000-0000-0000-0000\n        \n          \n        \n      \n\n      \n  \n  Born Analog, Made Digital\n  \n  Sophie Wu\n  \n  Overall, my work on this project focused on experimenting with ways of applying digital tools to transcribe text from archival images related to Virginia Woolf—largely her reading notebooks and financial records from the publishing company she ran, The Hogarth Press. Creating machine-readable versions of these images would hopefully allow for new and novel inquiry into Woolf, her reach, and the economics not only of the business she ran, but the impact that it had on the early 20th-century book industry at large.\n  My work over the summer was largely divided between these two bodies of images.\n  For her reading notebooks, I wrote code to both scrape and prepare images for automatic transcription, otherwise known as optical character recognition (OCR), on the platform Transkribus. This process mainly involved automatic scaling and cropping, binarization (turning the images black and white) and de-noising the images. Once the images were ready, they were fed into Transkribus, which automatically output its best guess at what Woolf had written. Given that these transcriptions were imperfect, I then fine tuned the model internal to Transkribus in an attempt to improve its performance. To do so, I compared the Transkribus output to a manual, verifiably correct, “ground-truth” transcription, and corrected the Transkribus output. Following this, I also put together a dataset of the final Transkribus transcriptions as compared to the “ground-truth” transcriptions, which could potentially be used for future fine-tuning of other types of OCR AI models.\n  \n  \n  \n  \n  \n  \n  \n  \n  Fig. 1. (a) An archival image of Woolf’s reading notebook.\n  \n  \n  \n  \n  \n  \n  \n  (b) The same image after processing\n  \n  \n  \n  \n  \n  \n  Figure 1: A comparison of an original archival image and the image after it’s been transformed in preparation for OCR.\n  \n  \n  \n  \n  \n  \n  Fig. 2. A depiction of working with Woolf’s notes in Transkribus. The transcription is output on the right.\n  \n  \n  For the financial records from Hogarth Press, I largely worked with Google’s Gemini, a large-language model (LLM) similar to ChatGPT. In doing this, I researched and experimented with the design of different prompts to best extract tabular and heterogenous data. In parallel, I also wrote code and modified computer vision algorithms to improve the accuracy of automatic image segmentation, which in turn makes Gemini’s transcriptions more accurate.\n  \n  \n  \n  Fig. 3. An archival image of a page from a Hogarth Press Orderbook\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  Fig. 4. (a) Dilate\n  \n  \n  \n  \n  \n  \n  \n  (b) Group\n  \n  \n  \n  \n  \n  \n  \n  (c) Boundaries\n  \n  \n  \n  \n  \n  \n  Figure 2: A depiction of the process used to detecting columns using computer vision techniques. The image is binarized and dilated to produce (a), from which the features are detected and grouped as in (b). The groups are then used to adjust the boundaries of each column in the image as visualized in (c).\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  Fig. 5. (a) Crop\n  \n  \n  \n  \n  \n  \n  \n  (b) Transcribed\n  \n  \n  \n  \n  \n  \n  Figure 3: Using the detected boundaries, we can automatically crop out the list of the purchaser names as shown in (a). This cropped image is them fed into Gemini, which returns the transcribed output in (b).\n  \n  \n  \n  All of this is put together in a pipeline that will allow for these financial records to be transcribed in bulk, and to reduce the human effort needed to transcribe these records, and otherwise correct them.\n  My work continues on in finetuning the process of extracting the other columns of information from each image, using additional image manipulation techniques, modified prompting, and leveraging LLMs for data cleaning.\n  \n  \n  \n  \n  \n  \n  \n  \n  Fig. 6. (a) Spliced\n  \n  \n  \n  \n  \n  \n  \n  (b) Transcribed\n  \n  \n  \n  \n  \n  \n  Figure 4: Experimenting with splicing images in (a) to remove columns that are “distracting” for Gemini to read. The resulting transcription of this spliced image is shown in (b).\n  \n  \n  \n  In all, my work focuses on bringing born-analog texts into the modern age in order to further novel and unique inquiry into Woolf’s work and its continued influence."
  }
]